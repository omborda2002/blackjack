# Blackjack Reinforcement Learning Agent

This project implements a self-learning Blackjack agent using **Q-Learning** and explores various strategies including **card counting** and **rule variations**. The agent learns optimal play policies through simulation and logs performance metrics for analysis.

---

## ğŸ“ Project Structure

```
blackjack_rl/
â”œâ”€â”€ blackjack_env.py              # Blackjack game environment
â”œâ”€â”€ q_learning_agent.py          # Q-learning agent with optional counting
â”œâ”€â”€ run_experiment.py            # Runs all strategy experiments
â”œâ”€â”€ utils.py                     # Logging, evaluation, saving/loading Q-tables
â”œâ”€â”€ plots.py                     # Generates reward/win rate plots and summary CSV
â”œâ”€â”€ requirements.txt             # Dependencies list
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ figures/                     # Output plots + summary CSV
â”‚   â”œâ”€â”€ cumulative_reward_plot.png
â”‚   â”œâ”€â”€ rolling_winrate_plot.png
â”‚   â””â”€â”€ strategy_summary.csv
â”œâ”€â”€ logs/                        # Training logs and Q-tables
â”‚   â”œâ”€â”€ *.csv
â”‚   â”œâ”€â”€ *_q_table.pkl
```

---

## ğŸš€ Getting Started

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Train All Strategies

```bash
python run_experiment.py
```

This will train and evaluate five strategies:

* Basic Strategy
* Point Count (Hi-Lo)
* Dealer Hits on Soft 17
* Double After Split Allowed
* Improved Point Count (custom weights)

### 3. Generate Plots & Summary Table

```bash
python plots.py
```

This will create:

* `figures/cumulative_reward_plot.png`
* `figures/rolling_winrate_plot.png`
* `figures/strategy_summary.csv`

---

## ğŸ“Š Evaluation Metrics

After each training run, the agent is evaluated on:

* **Average Reward per Game**
* **Win Rate (%)**
* **Profit per Hour (â‚¬)** (assuming 100 hands/hour)

These metrics appear both in console output and the summary CSV.

---

## ğŸ§  Strategy Modes Explained

| Strategy                       | Description                                         |
| ------------------------------ | --------------------------------------------------- |
| `basic_strategy`               | Pure state-based Q-learning (no memory)             |
| `point_count`                  | Adds Hi-Lo running count as a state feature         |
| `variation_hits_soft_17`       | Dealer hits on soft 17 rule tested                  |
| `variation_double_after_split` | Double-down allowed after splits                    |
| `improved_point_count`         | Uses custom weights (e.g. +2 for 5,6; -2 for 10,11) |

---

## ğŸ’¾ Reproducibility

* Logs are saved as `.csv` in `logs/`
* Q-tables are saved as `.pkl` files per strategy
* All results are deterministic based on random seed unless modified

---

## ğŸ“Œ Notes

* All agents use **epsilon-greedy** exploration with decay
* Bets are scaled based on the **true count** if enabled in the environment
* No external RL libraries were used (per exam rules)

---

## ğŸ‘¨â€ğŸ« Author

**Om Sanjaybhai Borda**
Technical University of Applied Sciences WÃ¼rzburg-Schweinfurt
Email: [om.borda@study.thws.de](mailto:om.borda@study.thws.de)

---

## ğŸ“œ License

This project is part of the Portfolio Exam 3 for *Reasoning and Decision Making under Uncertainty (Summer 2025)* and is submitted to Prof. Dr. Frank Deinzer.
